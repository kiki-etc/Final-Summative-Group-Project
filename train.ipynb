{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the appropriate command for your operating system, if required\n",
    "# No installation is reqiured on Google Colab / Kaggle notebooks\n",
    "\n",
    "# Linux / Binder / Windows (No GPU)\n",
    "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Linux / Windows (GPU)\n",
    "# pip install numpy matplotlib torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    " \n",
    "# MacOS (NO GPU)\n",
    "# !pip install numpy matplotlib torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.11/site-packages (10.2.0)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "required_packages = ['numpy', 'matplotlib', 'torch', 'torchvision', 'torchaudio', 'transformers', 'pillow', 'json']\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        install(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n",
      "{'artwork001.png': 'nomadic', 'artwork002.png': 'nomadic', 'artwork003.png': 'nomadic', 'artwork004.png': 'nomadic', 'artwork005.png': 'nomadic', 'artwork006.png': 'nomadic', 'artwork007.png': 'nomadic', 'artwork008.png': 'nomadic', 'artwork009.png': 'nomadic', 'artwork010.png': 'nomadic', 'artwork011.png': 'nomadic', 'artwork012.png': 'nomadic', 'artwork013.png': 'nomadic', 'artwork014.png': 'nomadic', 'artwork015.png': 'nomadic', 'artwork016.png': 'nomadic', 'artwork017.png': 'nomadic', 'artwork018.png': 'nomadic', 'artwork019.png': 'nomadic', 'artwork020.png': 'nomadic', 'artwork021.png': 'nomadic', 'artwork022.png': 'nomadic', 'artwork023.png': 'nomadic', 'artwork024.png': 'nomadic', 'artwork025.png': 'nomadic', 'artwork026.png': 'nomadic', 'artwork027.png': 'nomadic', 'artwork028.png': 'nomadic', 'artwork029.png': 'nomadic', 'artwork030.png': 'nomadic', 'artwork031.png': 'nomadic', 'artwork032.png': 'nomadic', 'artwork033.png': 'nomadic', 'artwork034.png': 'nomadic', 'artwork035.png': 'nomadic', 'artwork036.png': 'nomadic', 'artwork037.png': 'nomadic', 'artwork038.png': 'nomadic', 'artwork039.png': 'nomadic', 'artwork040.png': 'nomadic', 'artwork041.png': 'nomadic', 'artwork042.png': 'nomadic', 'artwork043.png': 'nomadic', 'artwork044.png': 'nomadic', 'artwork045.png': 'nomadic', 'artwork046.png': 'nomadic', 'artwork047.png': 'nomadic', 'artwork048.png': 'nomadic', 'artwork049.png': 'nomadic', 'artwork050.png': 'nomadic', 'artwork051.png': 'nomadic', 'artwork052.png': 'nomadic', 'artwork053.png': 'nomadic', 'artwork054.png': 'nomadic', 'artwork055.png': 'nomadic', 'artwork056.png': 'nomadic', 'artwork057.png': 'nomadic', 'artwork058.png': 'nomadic', 'artwork059.png': 'nomadic', 'artwork060.png': 'nomadic', 'artwork061.png': 'nomadic', 'artwork062.png': 'nomadic', 'artwork063.png': 'nomadic', 'artwork064.png': 'cubism', 'artwork065.png': 'cubism', 'artwork066.png': 'cubism', 'artwork067.png': 'cubism', 'artwork068.png': 'cubism', 'artwork069.png': 'cubism', 'artwork070.png': 'cubism', 'artwork071.png': 'cubism', 'artwork072.png': 'cubism', 'artwork073.png': 'cubism', 'artwork074.png': 'cubism', 'artwork075.png': 'cubism', 'artwork076.png': 'cubism', 'artwork077.png': 'cubism', 'artwork078.png': 'cubism', 'artwork079.png': 'cubism', 'artwork080.png': 'cubism', 'artwork081.png': 'cubism', 'artwork082.png': 'cubism', 'artwork083.png': 'cubism', 'artwork084.png': 'cubism', 'artwork085.png': 'print', 'artwork086.png': 'print', 'artwork087.png': 'print', 'artwork088.png': 'print', 'artwork089.png': 'print', 'artwork090.png': 'print', 'artwork091.png': 'print', 'artwork092.png': 'print', 'artwork093.png': 'print', 'artwork094.png': 'print', 'artwork095.png': 'print', 'artwork096.png': 'print', 'artwork097.png': 'print', 'artwork098.png': 'print', 'artwork099.png': 'print', 'artwork100.png': 'print', 'artwork101.png': 'print', 'artwork102.png': 'print', 'artwork103.png': 'print', 'artwork104.png': 'print', 'artwork105.png': 'print', 'artwork106.png': 'gray', 'artwork107.png': 'gray', 'artwork108.png': 'gray', 'artwork109.png': 'gray', 'artwork110.png': 'gray', 'artwork111.png': 'gray', 'artwork112.png': 'gray', 'artwork113.png': 'gray', 'artwork114.png': 'gray', 'artwork115.png': 'gray', 'artwork116.png': 'gray', 'artwork117.png': 'gray', 'artwork118.png': 'gray', 'artwork119.png': 'gray', 'artwork120.png': 'gray', 'artwork121.png': 'gray', 'artwork122.png': 'gray', 'artwork123.png': 'gray', 'artwork124.png': 'gray', 'artwork125.png': 'gray', 'artwork126.png': 'gray'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "\n",
    "base_dir = 'dataset'\n",
    "image_folders = ['nomadic', 'cubism', 'print', 'gray']\n",
    "output_image_dir = os.path.join(base_dir, 'images')\n",
    "output_text_dir = os.path.join(base_dir, 'text')\n",
    "\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(output_text_dir, exist_ok=True)\n",
    "\n",
    "filters = [\n",
    "    ('BLUR', ImageFilter.BLUR),\n",
    "    ('CONTOUR', ImageFilter.CONTOUR),\n",
    "    ('DETAIL', ImageFilter.DETAIL),\n",
    "    ('EDGE_ENHANCE', ImageFilter.EDGE_ENHANCE),\n",
    "    ('EDGE_ENHANCE_MORE', ImageFilter.EDGE_ENHANCE_MORE),\n",
    "    ('EMBOSS', ImageFilter.EMBOSS),\n",
    "    ('FIND_EDGES', ImageFilter.FIND_EDGES),\n",
    "    ('SHARPEN', ImageFilter.SHARPEN),\n",
    "    ('SMOOTH', ImageFilter.SMOOTH),\n",
    "    ('SMOOTH_MORE', ImageFilter.SMOOTH_MORE)\n",
    "]\n",
    "enhancements = [\n",
    "    ('BRIGHTNESS', ImageEnhance.Brightness, 1.2),\n",
    "    ('CONTRAST', ImageEnhance.Contrast, 1.2),\n",
    "    ('COLOR', ImageEnhance.Color, 1.2),\n",
    "    ('SHARPNESS', ImageEnhance.Sharpness, 1.2)\n",
    "]\n",
    "additional_filters = [\n",
    "    ('GAUSSIAN_BLUR', ImageFilter.GaussianBlur(radius=2)),\n",
    "    ('UNSHARP_MASK', ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3)),\n",
    "    ('MEDIAN_FILTER', ImageFilter.MedianFilter(size=3)),\n",
    "    ('MIN_FILTER', ImageFilter.MinFilter(size=3)),\n",
    "    ('MAX_FILTER', ImageFilter.MaxFilter(size=3)),\n",
    "    ('MODE_FILTER', ImageFilter.ModeFilter(size=3))\n",
    "]\n",
    "\n",
    "all_operations = filters + enhancements + additional_filters\n",
    "\n",
    "# Initialize the dictionary\n",
    "image_to_class = {}\n",
    "\n",
    "def apply_operations(image, output_dir, base_filename, counter, folder):\n",
    "    original_image_path = os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\")\n",
    "    image.save(original_image_path)\n",
    "    image_to_class[f\"{base_filename}{counter:03d}.png\"] = folder\n",
    "    counter += 1\n",
    "\n",
    "    for operation_name, operation in filters:\n",
    "        modified_image = image.filter(operation)\n",
    "        modified_image_path = os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\")\n",
    "        modified_image.save(modified_image_path)\n",
    "        image_to_class[f\"{base_filename}{counter:03d}.png\"] = folder\n",
    "        counter += 1\n",
    "\n",
    "    for operation_name, operation, factor in enhancements:\n",
    "        enhancer = operation(image)\n",
    "        modified_image = enhancer.enhance(factor)\n",
    "        modified_image_path = os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\")\n",
    "        modified_image.save(modified_image_path)\n",
    "        image_to_class[f\"{base_filename}{counter:03d}.png\"] = folder\n",
    "        counter += 1\n",
    "\n",
    "    for operation_name, operation in additional_filters:\n",
    "        modified_image = image.filter(operation)\n",
    "        modified_image_path = os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\")\n",
    "        modified_image.save(modified_image_path)\n",
    "        image_to_class[f\"{base_filename}{counter:03d}.png\"] = folder\n",
    "        counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "def clean_directory(directory):\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        if os.path.isfile(item_path) and '.' in item:\n",
    "            os.remove(item_path)\n",
    "\n",
    "image_counter = 1\n",
    "\n",
    "clean_directory(output_image_dir)\n",
    "clean_directory(output_text_dir)\n",
    "\n",
    "for folder in image_folders:\n",
    "    current_folder_path = os.path.join(base_dir, 'images', folder)\n",
    "    for filename in os.listdir(current_folder_path):\n",
    "        if filename.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'gif')):\n",
    "            image_path = os.path.join(current_folder_path, filename)\n",
    "            image = Image.open(image_path)\n",
    "            base_filename = \"artwork\"\n",
    "\n",
    "            image_counter = apply_operations(image, output_image_dir, base_filename, image_counter, folder)\n",
    "\n",
    "            # Create corresponding text file\n",
    "            for i in range(image_counter - 21, image_counter):\n",
    "                txt_filename = f\"{base_filename}{i:03d}.txt\"\n",
    "                with open(os.path.join(output_text_dir, txt_filename), 'w') as f:\n",
    "                    f.write(folder)\n",
    "\n",
    "print(\"Processing complete.\")\n",
    "print(image_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and DataLoader created successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_to_class, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.image_to_class = image_to_class\n",
    "        self.class_to_idx = {'nomadic': 0, 'cubism': 1, 'print': 2, 'gray': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        image = Image.open(os.path.join(self.image_dir, img_name))\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        class_name = self.image_to_class[img_name]\n",
    "        label = self.class_to_idx[class_name]\n",
    "        label = torch.eye(len(self.class_to_idx))[label]  # One-hot encode the label\n",
    "\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset = TextImageDataset(image_dir=output_image_dir, image_to_class=image_to_class, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "print(\"Dataset and DataLoader created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Defining the Generator and Discriminator networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define Generator Class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, text_dim, img_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.text_embedding = nn.Linear(text_dim, latent_dim)\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim * 2, 512, 4, 1, 0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text):\n",
    "        text_embedding = self.text_embedding(text).unsqueeze(2).unsqueeze(3)\n",
    "        text_embedding = text_embedding.expand(noise.size(0), -1, noise.size(2), noise.size(3))\n",
    "        x = torch.cat([noise, text_embedding], dim=1)\n",
    "        return self.gen(x)\n",
    "\n",
    "# Define Discriminator Class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, text_dim, hidden_dim, img_size=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def conv_output_size(size, kernel_size=4, stride=2, padding=1):\n",
    "            return (size - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        current_size = img_size\n",
    "        self.img_dis = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, hidden_dim // 4, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        current_size = conv_output_size(current_size, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.img_dis.add_module(\"conv2\", nn.Conv2d(hidden_dim // 4, hidden_dim // 2, 4, stride=2, padding=1))\n",
    "        self.img_dis.add_module(\"lrelu2\", nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_size = conv_output_size(current_size, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.img_dis.add_module(\"conv3\", nn.Conv2d(hidden_dim // 2, hidden_dim, 4, stride=2, padding=1))\n",
    "        self.img_dis.add_module(\"lrelu3\", nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_size = conv_output_size(current_size, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.text_embedding = nn.Linear(text_dim, hidden_dim * current_size * current_size)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim * 2, hidden_dim, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(hidden_dim, 1, 3, 1, 1),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        img_out = self.img_dis(img)\n",
    "        text_embedding = self.text_embedding(text.float()).view(text.size(0), self.hidden_dim, img_out.size(2), img_out.size(3))\n",
    "        x = torch.cat([img_out, text_embedding], dim=1)\n",
    "        return self.final(x).view(img.size(0), 1)  # Ensuring the output size is [batch_size, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataloader is already defined and initialized\n",
    "latent_dim = 100\n",
    "text_dim = 4  # Number of classes for one-hot encoding\n",
    "img_channels = 3  # Number of channels in the input image (RGB)\n",
    "hidden_dim = 512\n",
    "img_size = 128\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "generator = Generator(latent_dim, 4, 3).to(device)\n",
    "discriminator = Discriminator(3, 4, 512, img_size=128).to(device)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "dataset = TextImageDataset(image_dir=output_image_dir, image_to_class=image_to_class, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "The generator and discriminator are trained here using a loop.\n",
    "## Optimization\n",
    "The optimization is embedded within the training loop. The code uses Adam optimizer to update the weights of both the generator and the discriminator.\n",
    "## Evaluation\n",
    "Evaluation happens within the training loop where loss values for both the generator and the discriminator are printed every 100 steps. Additionally, generated sample images are saved at the end of each epoch to visualize the generator’s progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Training Function\n",
    "def train_gan(generator, discriminator, dataloader, epochs, lr, device):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optim_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            batch_size = images.size(0)\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optim_d.zero_grad()\n",
    "            outputs = discriminator(images, labels)\n",
    "            real_loss = criterion(outputs, real_labels)\n",
    "            real_loss.backward()\n",
    "\n",
    "            noise = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n",
    "            fake_images = generator(noise, labels)\n",
    "            outputs = discriminator(fake_images.detach(), labels)\n",
    "            fake_loss = criterion(outputs, fake_labels)\n",
    "            fake_loss.backward()\n",
    "            optim_d.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optim_g.zero_grad()\n",
    "            outputs = discriminator(fake_images, labels)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            g_loss.backward()\n",
    "            optim_g.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(dataloader)}], '\n",
    "                      f'D Loss: {real_loss.item() + fake_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample_noise = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n",
    "            sample_images = generator(sample_noise, labels)\n",
    "            os.makedirs('samples', exist_ok=True)\n",
    "            utils.save_image(sample_images, f'samples/sample_epoch_{epoch + 1}.png', nrow=8, normalize=True)\n",
    "\n",
    "    torch.save(generator.state_dict(), 'generator.pth')\n",
    "    torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
    "\n",
    "train_gan(generator, discriminator, dataloader, epochs, lr, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues faced with Matrix Multiplication\n",
    "During this project, several matrix multiplication issues arose primarily due to mismatched dimensions between the text embeddings and the input dimensions expected by the neural network layers. Dealing with this ***singlehandedly*** has proven to be a struggle.\n",
    "\n",
    "1. **Tokenization and Embedding Dimension Mismatch**:\n",
    "    - Initially, a tokenizer was used to convert text descriptions into token indices, followed by an embedding layer to transform these indices into dense vectors.\n",
    "    - This was originally due to the complexity of the dataset, which had comprehensive descriptions of each artwork. So the dataset was manually changed and reduced in size. Due to the current limited vocabulary (only four unique words), using pre-trained tokenizers led to unnecessary complexity and mismatches in embedding dimensions, causing errors in matrix multiplication during model training, which have persisted since changing from a complex tokenizer to even using a simple dicitonary.\n",
    "\n",
    "2. **Concatenation of Noise and Text Embeddings**:\n",
    "    - In the generator, noise vectors and text embeddings were concatenated before being fed into the model. Inconsistent dimensions between the noise vector (latent dimension) and the text embeddings often resulted in mismatched tensor shapes.\n",
    "    - This mismatch led to runtime errors, particularly during the forward pass where matrix multiplication operations were performed.\n",
    "\n",
    "3. **Manual Calculation of Flattened Image Size**:\n",
    "    - The discriminator network involved convolutional layers followed by fully connected layers, requiring precise calculation of the flattened image size after the convolutions.\n",
    "    - Any miscalculation in these dimensions led to shape mismatches during the concatenation of image features and text embeddings, causing matrix multiplication errors.\n",
    "\n",
    "4. **Inconsistent Handling of Text Embeddings**:\n",
    "    - Variations in how text embeddings were handled across different components of the model (e.g., direct embeddings, linear transformation) introduced inconsistencies.\n",
    "    - These inconsistencies manifested as shape mismatches during the integration of text embeddings with image features or noise vectors.\n",
    "\n",
    "5. **Solution with Manual Embeddings**:\n",
    "    - To resolve these issues, manual embeddings for the limited vocabulary were implemented. This approach involved defining fixed-size embeddings for each unique word, ensuring consistent dimensions. The issue still persists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
