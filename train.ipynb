{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the appropriate command for your operating system, if required\n",
    "# No installation is reqiured on Google Colab / Kaggle notebooks\n",
    "\n",
    "# Linux / Binder / Windows (No GPU)\n",
    "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Linux / Windows (GPU)\n",
    "# pip install numpy matplotlib torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    " \n",
    "# MacOS (NO GPU)\n",
    "# !pip install numpy matplotlib torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.11/site-packages (10.2.0)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "required_packages = ['numpy', 'matplotlib', 'torch', 'torchvision', 'torchaudio', 'transformers', 'pillow', 'json']\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        install(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n",
      "{'artwork001.png': 'nomadic', 'artwork002.png': 'nomadic', 'artwork003.png': 'nomadic', 'artwork004.png': 'nomadic', 'artwork005.png': 'nomadic', 'artwork006.png': 'nomadic', 'artwork007.png': 'nomadic', 'artwork008.png': 'nomadic', 'artwork009.png': 'nomadic', 'artwork010.png': 'nomadic', 'artwork011.png': 'nomadic', 'artwork012.png': 'nomadic', 'artwork013.png': 'nomadic', 'artwork014.png': 'nomadic', 'artwork015.png': 'nomadic', 'artwork016.png': 'nomadic', 'artwork017.png': 'nomadic', 'artwork018.png': 'nomadic', 'artwork019.png': 'nomadic', 'artwork020.png': 'nomadic', 'artwork021.png': 'nomadic', 'artwork022.png': 'nomadic', 'artwork023.png': 'nomadic', 'artwork024.png': 'nomadic', 'artwork025.png': 'nomadic', 'artwork026.png': 'nomadic', 'artwork027.png': 'nomadic', 'artwork028.png': 'nomadic', 'artwork029.png': 'nomadic', 'artwork030.png': 'nomadic', 'artwork031.png': 'nomadic', 'artwork032.png': 'nomadic', 'artwork033.png': 'nomadic', 'artwork034.png': 'nomadic', 'artwork035.png': 'nomadic', 'artwork036.png': 'nomadic', 'artwork037.png': 'nomadic', 'artwork038.png': 'nomadic', 'artwork039.png': 'nomadic', 'artwork040.png': 'nomadic', 'artwork041.png': 'nomadic', 'artwork042.png': 'nomadic', 'artwork043.png': 'nomadic', 'artwork044.png': 'nomadic', 'artwork045.png': 'nomadic', 'artwork046.png': 'nomadic', 'artwork047.png': 'nomadic', 'artwork048.png': 'nomadic', 'artwork049.png': 'nomadic', 'artwork050.png': 'nomadic', 'artwork051.png': 'nomadic', 'artwork052.png': 'nomadic', 'artwork053.png': 'nomadic', 'artwork054.png': 'nomadic', 'artwork055.png': 'nomadic', 'artwork056.png': 'nomadic', 'artwork057.png': 'nomadic', 'artwork058.png': 'nomadic', 'artwork059.png': 'nomadic', 'artwork060.png': 'nomadic', 'artwork061.png': 'nomadic', 'artwork062.png': 'nomadic', 'artwork063.png': 'nomadic', 'artwork064.png': 'cubism', 'artwork065.png': 'cubism', 'artwork066.png': 'cubism', 'artwork067.png': 'cubism', 'artwork068.png': 'cubism', 'artwork069.png': 'cubism', 'artwork070.png': 'cubism', 'artwork071.png': 'cubism', 'artwork072.png': 'cubism', 'artwork073.png': 'cubism', 'artwork074.png': 'cubism', 'artwork075.png': 'cubism', 'artwork076.png': 'cubism', 'artwork077.png': 'cubism', 'artwork078.png': 'cubism', 'artwork079.png': 'cubism', 'artwork080.png': 'cubism', 'artwork081.png': 'cubism', 'artwork082.png': 'cubism', 'artwork083.png': 'cubism', 'artwork084.png': 'cubism', 'artwork085.png': 'print', 'artwork086.png': 'print', 'artwork087.png': 'print', 'artwork088.png': 'print', 'artwork089.png': 'print', 'artwork090.png': 'print', 'artwork091.png': 'print', 'artwork092.png': 'print', 'artwork093.png': 'print', 'artwork094.png': 'print', 'artwork095.png': 'print', 'artwork096.png': 'print', 'artwork097.png': 'print', 'artwork098.png': 'print', 'artwork099.png': 'print', 'artwork100.png': 'print', 'artwork101.png': 'print', 'artwork102.png': 'print', 'artwork103.png': 'print', 'artwork104.png': 'print', 'artwork105.png': 'print', 'artwork106.png': 'gray', 'artwork107.png': 'gray', 'artwork108.png': 'gray', 'artwork109.png': 'gray', 'artwork110.png': 'gray', 'artwork111.png': 'gray', 'artwork112.png': 'gray', 'artwork113.png': 'gray', 'artwork114.png': 'gray', 'artwork115.png': 'gray', 'artwork116.png': 'gray', 'artwork117.png': 'gray', 'artwork118.png': 'gray', 'artwork119.png': 'gray', 'artwork120.png': 'gray', 'artwork121.png': 'gray', 'artwork122.png': 'gray', 'artwork123.png': 'gray', 'artwork124.png': 'gray', 'artwork125.png': 'gray', 'artwork126.png': 'gray'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "\n",
    "base_dir = 'dataset'\n",
    "image_folders = ['nomadic', 'cubism', 'print', 'gray']\n",
    "output_image_dir = os.path.join(base_dir, 'images')\n",
    "output_text_dir = os.path.join(base_dir, 'text')\n",
    "\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(output_text_dir, exist_ok=True)\n",
    "\n",
    "filters = [\n",
    "    ('BLUR', ImageFilter.BLUR),\n",
    "    ('CONTOUR', ImageFilter.CONTOUR),\n",
    "    ('DETAIL', ImageFilter.DETAIL),\n",
    "    ('EDGE_ENHANCE', ImageFilter.EDGE_ENHANCE),\n",
    "    ('EDGE_ENHANCE_MORE', ImageFilter.EDGE_ENHANCE_MORE),\n",
    "    ('EMBOSS', ImageFilter.EMBOSS),\n",
    "    ('FIND_EDGES', ImageFilter.FIND_EDGES),\n",
    "    ('SHARPEN', ImageFilter.SHARPEN),\n",
    "    ('SMOOTH', ImageFilter.SMOOTH),\n",
    "    ('SMOOTH_MORE', ImageFilter.SMOOTH_MORE)\n",
    "]\n",
    "enhancements = [\n",
    "    ('BRIGHTNESS', ImageEnhance.Brightness, 1.2),\n",
    "    ('CONTRAST', ImageEnhance.Contrast, 1.2),\n",
    "    ('COLOR', ImageEnhance.Color, 1.2),\n",
    "    ('SHARPNESS', ImageEnhance.Sharpness, 1.2)\n",
    "]\n",
    "additional_filters = [\n",
    "    ('GAUSSIAN_BLUR', ImageFilter.GaussianBlur(radius=2)),\n",
    "    ('UNSHARP_MASK', ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3)),\n",
    "    ('MEDIAN_FILTER', ImageFilter.MedianFilter(size=3)),\n",
    "    ('MIN_FILTER', ImageFilter.MinFilter(size=3)),\n",
    "    ('MAX_FILTER', ImageFilter.MaxFilter(size=3)),\n",
    "    ('MODE_FILTER', ImageFilter.ModeFilter(size=3))\n",
    "]\n",
    "\n",
    "all_operations = filters + enhancements + additional_filters\n",
    "\n",
    "# Initialize the dictionary\n",
    "image_to_class = {}\n",
    "\n",
    "def apply_operations(image, output_dir, base_filename, counter, folder):\n",
    "    original_image_path = os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\")\n",
    "    image.save(original_image_path)\n",
    "    image_to_class[f\"{base_filename}{counter:03d}.png\"] = folder\n",
    "    counter += 1\n",
    "\n",
    "    for operation_name, operation in filters:\n",
    "        modified_image = image.filter(operation)\n",
    "        modified_image_path = os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\")\n",
    "        modified_image.save(modified_image_path)\n",
    "        image_to_class[f\"{base_filename}{counter:03d}.png\"] = folder\n",
    "        counter += 1\n",
    "\n",
    "    for operation_name, operation, factor in enhancements:\n",
    "        enhancer = operation(image)\n",
    "        modified_image = enhancer.enhance(factor)\n",
    "        modified_image_path = os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\")\n",
    "        modified_image.save(modified_image_path)\n",
    "        image_to_class[f\"{base_filename}{counter:03d}.png\"] = folder\n",
    "        counter += 1\n",
    "\n",
    "    for operation_name, operation in additional_filters:\n",
    "        modified_image = image.filter(operation)\n",
    "        modified_image_path = os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\")\n",
    "        modified_image.save(modified_image_path)\n",
    "        image_to_class[f\"{base_filename}{counter:03d}.png\"] = folder\n",
    "        counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "def clean_directory(directory):\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        if os.path.isfile(item_path) and '.' in item:\n",
    "            os.remove(item_path)\n",
    "\n",
    "image_counter = 1\n",
    "\n",
    "clean_directory(output_image_dir)\n",
    "clean_directory(output_text_dir)\n",
    "\n",
    "for folder in image_folders:\n",
    "    current_folder_path = os.path.join(base_dir, 'images', folder)\n",
    "    for filename in os.listdir(current_folder_path):\n",
    "        if filename.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'gif')):\n",
    "            image_path = os.path.join(current_folder_path, filename)\n",
    "            image = Image.open(image_path)\n",
    "            base_filename = \"artwork\"\n",
    "\n",
    "            image_counter = apply_operations(image, output_image_dir, base_filename, image_counter, folder)\n",
    "\n",
    "            # Create corresponding text file\n",
    "            for i in range(image_counter - 21, image_counter):\n",
    "                txt_filename = f\"{base_filename}{i:03d}.txt\"\n",
    "                with open(os.path.join(output_text_dir, txt_filename), 'w') as f:\n",
    "                    f.write(folder)\n",
    "\n",
    "print(\"Processing complete.\")\n",
    "print(image_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and DataLoader created successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_to_class, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.image_to_class = image_to_class\n",
    "        self.class_to_idx = {'nomadic': 0, 'cubism': 1, 'print': 2, 'gray': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        image = Image.open(os.path.join(self.image_dir, img_name))\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        class_name = self.image_to_class[img_name]\n",
    "        label = self.class_to_idx[class_name]\n",
    "        label = torch.eye(len(self.class_to_idx))[label]  # One-hot encode the label\n",
    "\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset = TextImageDataset(image_dir=output_image_dir, image_to_class=image_to_class, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "print(\"Dataset and DataLoader created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Defining the Generator and Discriminator networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define Generator Class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, text_dim, img_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.text_embedding = nn.Linear(text_dim, latent_dim)\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim * 2, 512, 4, 1, 0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text):\n",
    "        text_embedding = self.text_embedding(text).unsqueeze(2).unsqueeze(3)\n",
    "        text_embedding = text_embedding.expand(noise.size(0), -1, noise.size(2), noise.size(3))\n",
    "        x = torch.cat([noise, text_embedding], dim=1)\n",
    "        return self.gen(x)\n",
    "\n",
    "# Define Discriminator Class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, text_dim, hidden_dim, img_size=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def conv_output_size(size, kernel_size=4, stride=2, padding=1):\n",
    "            return (size - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        current_size = img_size\n",
    "        self.img_dis = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, hidden_dim // 4, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        current_size = conv_output_size(current_size, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.img_dis.add_module(\"conv2\", nn.Conv2d(hidden_dim // 4, hidden_dim // 2, 4, stride=2, padding=1))\n",
    "        self.img_dis.add_module(\"lrelu2\", nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_size = conv_output_size(current_size, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.img_dis.add_module(\"conv3\", nn.Conv2d(hidden_dim // 2, hidden_dim, 4, stride=2, padding=1))\n",
    "        self.img_dis.add_module(\"lrelu3\", nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_size = conv_output_size(current_size, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.text_embedding = nn.Linear(text_dim, hidden_dim * current_size * current_size)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim + hidden_dim, hidden_dim, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(hidden_dim, 1, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        img_out = self.img_dis(img)\n",
    "        text_embedding = self.text_embedding(text.float()).view(text.size(0), -1, img_out.size(2), img_out.size(3))\n",
    "        x = torch.cat([img_out, text_embedding], dim=1)\n",
    "        return self.final(x).view(img.size(0), -1)  # Ensuring the output size is [batch_size, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_dim = 4  # Number of classes for one-hot encoding\n",
    "# latent_dim = 100\n",
    "# hidden_dim = 512\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# generator = Generator(latent_dim, text_dim, 3).to(device)\n",
    "# discriminator = Discriminator(3, text_dim, hidden_dim, img_size=128).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "The generator and discriminator are trained here using a loop.\n",
    "## Optimization\n",
    "The optimization is embedded within the training loop. The code uses Adam optimizer to update the weights of both the generator and the discriminator.\n",
    "## Evaluation\n",
    "Evaluation happens within the training loop where loss values for both the generator and the discriminator are printed every 100 steps. Additionally, generated sample images are saved at the end of each epoch to visualize the generator’s progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([3, 1])) must be the same as input size (torch.Size([3, 256]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TextImageDataset(image_dir\u001b[38;5;241m=\u001b[39moutput_image_dir, image_to_class\u001b[38;5;241m=\u001b[39mimage_to_class, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     67\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 69\u001b[0m train_gan(generator, discriminator, dataloader, epochs, lr, device)\n",
      "Cell \u001b[0;32mIn[16], line 28\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, dataloader, epochs, lr, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m optim_d\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m discriminator(images, labels)\n\u001b[0;32m---> 28\u001b[0m real_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, real_labels)\n\u001b[1;32m     29\u001b[0m real_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, latent_dim, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:734\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target,\n\u001b[1;32m    735\u001b[0m                                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    736\u001b[0m                                               pos_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_weight,\n\u001b[1;32m    737\u001b[0m                                               reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3242\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3239\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([3, 1])) must be the same as input size (torch.Size([3, 256]))"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision.utils as utils\n",
    "import os\n",
    "\n",
    "# Training Function\n",
    "epochs = 50  # number of epochs\n",
    "lr = 0.0001  # learning rate\n",
    "latent_dim = 100  # latent dimension for the generator\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, epochs, lr, device):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optim_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            batch_size = images.size(0)\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optim_d.zero_grad()\n",
    "            outputs = discriminator(images, labels)\n",
    "            real_loss = criterion(outputs, real_labels)\n",
    "            real_loss.backward()\n",
    "\n",
    "            noise = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n",
    "            fake_images = generator(noise, labels)\n",
    "            outputs = discriminator(fake_images.detach(), labels)\n",
    "            fake_loss = criterion(outputs, fake_labels)\n",
    "            fake_loss.backward()\n",
    "            optim_d.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optim_g.zero_grad()\n",
    "            outputs = discriminator(fake_images, labels)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            g_loss.backward()\n",
    "            optim_g.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(dataloader)}], '\n",
    "                      f'D Loss: {real_loss.item() + fake_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample_noise = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n",
    "            sample_images = generator(sample_noise, labels)\n",
    "            os.makedirs('samples', exist_ok=True)\n",
    "            utils.save_image(sample_images, f'samples/sample_epoch_{epoch + 1}.png', nrow=8, normalize=True)\n",
    "\n",
    "    torch.save(generator.state_dict(), 'generator.pth')\n",
    "    torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
    "\n",
    "# Assuming dataloader is already defined and initialized\n",
    "generator = Generator(latent_dim, 4, 3).to(device)\n",
    "discriminator = Discriminator(3, 4, 512, img_size=128).to(device)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "dataset = TextImageDataset(image_dir=output_image_dir, image_to_class=image_to_class, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "train_gan(generator, discriminator, dataloader, epochs, lr, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
