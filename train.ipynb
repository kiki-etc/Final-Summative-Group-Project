{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the appropriate command for your operating system, if required\n",
    "# No installation is reqiured on Google Colab / Kaggle notebooks\n",
    "\n",
    "# Linux / Binder / Windows (No GPU)\n",
    "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Linux / Windows (GPU)\n",
    "# pip install numpy matplotlib torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    " \n",
    "# MacOS (NO GPU)\n",
    "# !pip install numpy matplotlib torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "required_packages = ['numpy', 'matplotlib', 'torch', 'torchvision', 'torchaudio', 'transformers', 'pillow']\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        install(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "image_dir = 'dataset/images'\n",
    "files = os.listdir(image_dir)  # list of all files in the directory\n",
    "\n",
    "counter = 1\n",
    "\n",
    "# pattern to match files already in the `artworkXXX.ext` format\n",
    "pattern = re.compile(r'artwork\\d{3}\\.\\w+')\n",
    "\n",
    "for file_name in files:\n",
    "    file_ext = os.path.splitext(file_name)[1]\n",
    "    \n",
    "    old_file_path = os.path.join(image_dir, file_name)\n",
    "    new_file_path = old_file_path\n",
    "    \n",
    "    if not pattern.match(file_name):\n",
    "        new_file_name = f'artwork{counter:03d}{file_ext}'\n",
    "        new_file_path = os.path.join(image_dir, new_file_name)\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        counter += 1\n",
    "    \n",
    "    try:\n",
    "        # Open the image\n",
    "        with Image.open(new_file_path) as img:\n",
    "            # Check if the image size is not 128x128\n",
    "            if img.size != (128, 128):\n",
    "                img_resized = img.resize((128, 128))\n",
    "                img_resized.save(new_file_path)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Skipping invalid file: {file_name}, error: {e}\")\n",
    "\n",
    "print(\"Renaming and resizing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import UnidentifiedImageError\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# custom dataset\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, text_dir, image_dir, transform=None):\n",
    "        self.text_dir = text_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.text_files = sorted([f for f in os.listdir(text_dir) if f.endswith('.txt')])\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.valid_image_files = self.filter_valid_images()\n",
    "        \n",
    "        # Debugging prints\n",
    "        print(f\"Number of images before filtering: {len(self.image_files)}\")\n",
    "        print(f\"Number of valid images: {len(self.valid_image_files)}\")\n",
    "        print(f\"Number of text files: {len(self.text_files)}\")\n",
    "        \n",
    "        if len(self.text_files) != len(self.valid_image_files):\n",
    "            print(\"Text files:\", self.text_files)\n",
    "            print(\"Valid image files:\", self.valid_image_files)\n",
    "            raise ValueError(\"Number of text files does not match number of valid image files.\")\n",
    "\n",
    "    def filter_valid_images(self):\n",
    "        valid_files = []\n",
    "        for img_name in self.image_files:\n",
    "            img_path = os.path.join(self.image_dir, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img.verify()  # Verify that it is, in fact, an image\n",
    "                valid_files.append(img_name)\n",
    "            except (UnidentifiedImageError, IOError):\n",
    "                print(f\"Invalid image file: {img_path}\")\n",
    "        return valid_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.valid_image_files[idx])\n",
    "        image = Image.open(img_name)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        txt_name = os.path.join(self.text_dir, self.text_files[idx])\n",
    "        description = self.read_text_file(txt_name)\n",
    "        \n",
    "        tokens = self.tokenizer(description, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        return image, tokens['input_ids'].squeeze(), tokens['attention_mask'].squeeze()\n",
    "\n",
    "    def read_text_file(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='latin1') as f:\n",
    "                return f.read()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Adjusted to match the resized images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize expects 3 channels (RGB)\n",
    "])\n",
    "\n",
    "# creating dataset and dataloader\n",
    "dataset = TextImageDataset(text_dir='dataset/text', image_dir='dataset/images', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(\"Dataset and DataLoader created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, text_dim, img_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.text_embedding = nn.Linear(text_dim, latent_dim)\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim * 2, 512, 4, 1, 0),  # latent_dim doubled for concatenation\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text):\n",
    "        text = text.float()  # Convert text tensor to float32\n",
    "        text_embedding = self.text_embedding(text)\n",
    "        x = torch.cat([noise, text_embedding], dim=1)\n",
    "        x = x.unsqueeze(2).unsqueeze(3)\n",
    "        return self.gen(x)  # Forward pass through generator\n",
    "\n",
    "# defining the Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, text_dim, hidden_dim, img_size=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Calculate the size of the feature maps after each convolution layer\n",
    "        def conv_output_size(size, kernel_size=4, stride=2, padding=1):\n",
    "            return (size - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        current_size = img_size\n",
    "        self.img_dis = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, hidden_dim // 4, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        current_size = conv_output_size(current_size)\n",
    "\n",
    "        self.img_dis.add_module(\"conv2\", nn.Conv2d(hidden_dim // 4, hidden_dim // 2, 4, stride=2, padding=1))\n",
    "        self.img_dis.add_module(\"lrelu2\", nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_size = conv_output_size(current_size)\n",
    "\n",
    "        self.img_dis.add_module(\"conv3\", nn.Conv2d(hidden_dim // 2, hidden_dim, 4, stride=2, padding=1))\n",
    "        self.img_dis.add_module(\"lrelu3\", nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_size = conv_output_size(current_size)\n",
    "\n",
    "        # Calculate the flattened image size after convolutions\n",
    "        self.flattened_img_size = hidden_dim * (current_size * current_size)\n",
    "\n",
    "        # Text embedding layer to match the flattened image size\n",
    "        self.text_embedding = nn.Linear(text_dim, self.flattened_img_size)\n",
    "        self.final = nn.Linear(self.flattened_img_size * 2, 1)  # Concatenate image and text embeddings\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        img_out = self.img_dis(img).view(img.size(0), -1)  # Flatten the image output\n",
    "        text_embedding = self.text_embedding(text.float())  # Convert text to float and embed\n",
    "        x = torch.cat([img_out, text_embedding], dim=1)\n",
    "        return self.final(x)\n",
    "\n",
    "text_dim = 768  # BERT base model output dimension\n",
    "latent_dim = 100\n",
    "hidden_dim = 128  # adjusted hidden dimension\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator = Generator(latent_dim, text_dim, 3).to(device)\n",
    "discriminator = Discriminator(3, text_dim, hidden_dim, img_size=128).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "The generator and discriminator are trained here using a loop.\n",
    "## Optimization\n",
    "The optimization is embedded within the training loop. The code uses Adam optimizer to update the weights of both the generator and the discriminator.\n",
    "## Evaluation\n",
    "Evaluation happens within the training loop where loss values for both the generator and the discriminator are printed every 100 steps. Additionally, generated sample images are saved at the end of each epoch to visualize the generator’s progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# parameter setting\n",
    "image_size = 128  # Adjusted image size\n",
    "batch_size = 128\n",
    "epochs = 25\n",
    "lr = 0.0002\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, epochs, lr, device):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optim_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, text_ids, text_mask) in enumerate(dataloader):\n",
    "            batch_size = images.size(0)\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)  # Define real labels\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)  # Define fake labels\n",
    "\n",
    "            images = images.to(device)\n",
    "            text_ids = text_ids.to(device)\n",
    "            text_mask = text_mask.to(device)\n",
    "\n",
    "            optim_d.zero_grad()\n",
    "            outputs = discriminator(images, text_ids)\n",
    "            real_loss = criterion(outputs, real_labels)\n",
    "            real_loss.backward()\n",
    "\n",
    "            noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_images = generator(noise, text_ids)\n",
    "            outputs = discriminator(fake_images.detach(), text_ids)\n",
    "            fake_loss = criterion(outputs, fake_labels)\n",
    "            fake_loss.backward()\n",
    "            optim_d.step()\n",
    "\n",
    "            optim_g.zero_grad()\n",
    "            outputs = discriminator(fake_images, text_ids)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            g_loss.backward()\n",
    "            optim_g.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(dataloader)}], '\n",
    "                      f'D Loss: {real_loss.item() + fake_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample_noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "            sample_images = generator(sample_noise, text_ids)\n",
    "            os.makedirs('samples', exist_ok=True)\n",
    "            utils.save_image(sample_images, f'samples/sample_epoch_{epoch + 1}.png', nrow=8, normalize=True)\n",
    "\n",
    "    torch.save(generator.state_dict(), 'generator.pth')\n",
    "    torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
    "\n",
    "# training the models\n",
    "train_gan(generator, discriminator, dataloader, epochs, lr, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
