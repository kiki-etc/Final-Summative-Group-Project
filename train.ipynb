{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the appropriate command for your operating system, if required\n",
    "# No installation is reqiured on Google Colab / Kaggle notebooks\n",
    "\n",
    "# Linux / Binder / Windows (No GPU)\n",
    "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Linux / Windows (GPU)\n",
    "# pip install numpy matplotlib torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    " \n",
    "# MacOS (NO GPU)\n",
    "# !pip install numpy matplotlib torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.11/site-packages (10.2.0)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "required_packages = ['numpy', 'matplotlib', 'torch', 'torchvision', 'torchaudio', 'transformers', 'pillow', 'json']\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        install(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating a local tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized keywords: {'abstract': 0, 'abstract swirls': 1, 'africa map': 2, 'avif': 3, 'back': 4, 'big head': 5, 'birds': 6, 'black': 7, 'black and white': 8, 'blob': 9, 'blue': 10, 'bluee': 11, 'blues': 12, 'boy': 13, 'browm': 14, 'brown': 15, 'circles': 16, 'colorful': 17, 'colorfull': 18, 'colorufl': 19, 'community gathering': 20, 'contemporary': 21, 'cool': 22, 'cubism': 23, 'cultural': 24, 'dance': 25, 'dark': 26, 'detailed': 27, 'dull': 28, 'egyptian': 29, 'elephant': 30, 'environmental': 31, 'expressionism': 32, 'face': 33, 'family': 34, 'feminism': 35, 'feminist': 36, 'festival': 37, 'figurative': 38, 'flat': 39, 'flora': 40, 'folk': 41, 'geometric': 42, 'geometric background': 43, 'girl': 44, 'grass': 45, 'green': 46, 'group': 47, 'group gathering': 48, 'group portrait': 49, 'hands': 50, 'house': 51, 'individual': 52, 'indiviual': 53, 'landscape': 54, 'line art': 55, 'lines': 56, 'man': 57, 'market': 58, 'mask': 59, 'men': 60, 'mess': 61, 'minimalist': 62, 'modern': 63, 'monochrome': 64, 'music': 65, 'neutral': 66, 'nomadic': 67, 'orange': 68, 'pastels': 69, 'patern': 70, 'pattern': 71, 'patterns': 72, 'portait boy': 73, 'portrait': 74, 'purple': 75, 'realistic': 76, 'realistic colorful': 77, 'realistic portrait': 78, 'red': 79, 'red orange': 80, 'reliastic': 81, 'repeated': 82, 'ritual': 83, 'round': 84, 'royalty': 85, 'rural': 86, 'shapes': 87, 'side profile': 88, 'silhouette': 89, 'smooth': 90, 'spiritual': 91, 'stick figures': 92, 'surrealism': 93, 'swirls': 94, 'symbolic': 95, 'textured': 96, 'traditional': 97, 'tribal': 98, 'two sides': 99, 'two-dimensional': 100, 'urban': 101, 'vibrant': 102, 'village': 103, 'warm': 104, 'white': 105, 'wildlife': 106, 'wmoan': 107, 'woman': 108, 'women': 109, 'yello': 110, 'yellow': 111}\n"
     ]
    }
   ],
   "source": [
    "def create_tokenizer(text_dir):\n",
    "    # Extract unique keywords\n",
    "    unique_words = set()\n",
    "    text_files = [f for f in os.listdir(text_dir) if f.endswith('.txt')]\n",
    "    for file_name in text_files:\n",
    "        file_path = os.path.join(text_dir, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read().lower()\n",
    "            words = re.split(r',\\s*', content)\n",
    "            unique_words.update(words)\n",
    "    \n",
    "    # Create a mapping from keywords to indices\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(sorted(unique_words))}\n",
    "    \n",
    "    # Save the tokenizer for later use\n",
    "    with open('tokenizer.json', 'w') as f:\n",
    "        json.dump(word_to_idx, f)\n",
    "    \n",
    "    return word_to_idx\n",
    "\n",
    "word_to_idx = create_tokenizer('dataset/text')\n",
    "print(f\"Tokenized keywords: {word_to_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'dataset'\n",
    "image_folders = ['nomadic', 'cubism', 'print', 'gray']\n",
    "output_image_dir = os.path.join(base_dir, 'images')\n",
    "output_text_dir = os.path.join(base_dir, 'text')\n",
    "\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(output_text_dir, exist_ok=True)\n",
    "\n",
    "# Define the list of filters and enhancements\n",
    "filters = [\n",
    "    ('BLUR', ImageFilter.BLUR),\n",
    "    ('CONTOUR', ImageFilter.CONTOUR),\n",
    "    ('DETAIL', ImageFilter.DETAIL),\n",
    "    ('EDGE_ENHANCE', ImageFilter.EDGE_ENHANCE),\n",
    "    ('EDGE_ENHANCE_MORE', ImageFilter.EDGE_ENHANCE_MORE),\n",
    "    ('EMBOSS', ImageFilter.EMBOSS),\n",
    "    ('FIND_EDGES', ImageFilter.FIND_EDGES),\n",
    "    ('SHARPEN', ImageFilter.SHARPEN),\n",
    "    ('SMOOTH', ImageFilter.SMOOTH),\n",
    "    ('SMOOTH_MORE', ImageFilter.SMOOTH_MORE)\n",
    "]\n",
    "\n",
    "enhancements = [\n",
    "    ('BRIGHTNESS', ImageEnhance.Brightness, 1.5),  # Increase brightness by 1.5\n",
    "    ('CONTRAST', ImageEnhance.Contrast, 1.5),  # Increase contrast by 1.5\n",
    "    ('COLOR', ImageEnhance.Color, 1.5),  # Increase color by 1.5\n",
    "    ('SHARPNESS', ImageEnhance.Sharpness, 2.0)  # Increase sharpness by 2.0\n",
    "]\n",
    "\n",
    "additional_filters = [\n",
    "    ('GAUSSIAN_BLUR', ImageFilter.GaussianBlur(radius=2)),\n",
    "    ('UNSHARP_MASK', ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3)),\n",
    "    ('MEDIAN_FILTER', ImageFilter.MedianFilter(size=3)),\n",
    "    ('MIN_FILTER', ImageFilter.MinFilter(size=3)),\n",
    "    ('MAX_FILTER', ImageFilter.MaxFilter(size=3)),\n",
    "    ('MODE_FILTER', ImageFilter.ModeFilter(size=3))\n",
    "]\n",
    "\n",
    "all_operations = filters + enhancements + additional_filters\n",
    "\n",
    "# Function to apply filters and enhancements\n",
    "def apply_operations(image, output_dir, base_filename, counter):\n",
    "    original_image_path = os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\")\n",
    "    image.save(original_image_path)\n",
    "    counter += 1\n",
    "\n",
    "    for operation_name, operation in filters:\n",
    "        modified_image = image.filter(operation)\n",
    "        modified_image.save(os.path.join(output_dir, f\"{base_filename}{counter:03d}_{operation_name}.png\"))\n",
    "        counter += 1\n",
    "\n",
    "    for operation_name, operation, factor in enhancements:\n",
    "        enhancer = operation(image)\n",
    "        modified_image = enhancer.enhance(factor)\n",
    "        modified_image.save(os.path.join(output_dir, f\"{base_filename}{counter:03d}_{operation_name}.png\"))\n",
    "        counter += 1\n",
    "\n",
    "    for operation_name, operation in additional_filters:\n",
    "        modified_image = image.filter(operation)\n",
    "        modified_image.save(os.path.join(output_dir, f\"{base_filename}{counter:03d}_{operation_name}.png\"))\n",
    "        counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "# Initialize the counter\n",
    "image_counter = 1\n",
    "\n",
    "# Process each folder\n",
    "for folder in image_folders:\n",
    "    current_folder_path = os.path.join(base_dir, 'images', folder)\n",
    "    for filename in os.listdir(current_folder_path):\n",
    "        if filename.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'gif')):\n",
    "            image_path = os.path.join(current_folder_path, filename)\n",
    "            image = Image.open(image_path)\n",
    "            base_filename = \"artwork\"\n",
    "\n",
    "            # Apply operations and save images\n",
    "            image_counter = apply_operations(image, output_image_dir, base_filename, image_counter)\n",
    "\n",
    "            # Create corresponding text file\n",
    "            for i in range(image_counter - 21, image_counter):\n",
    "                txt_filename = f\"{base_filename}{i:03d}.txt\"\n",
    "                with open(os.path.join(output_text_dir, txt_filename), 'w') as f:\n",
    "                    f.write(folder)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming and resizing completed.\n"
     ]
    }
   ],
   "source": [
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "image_dir = 'dataset/images'\n",
    "files = os.listdir(image_dir)  # list of all files in the directory\n",
    "\n",
    "counter = 1\n",
    "\n",
    "# pattern to match files already in the `artworkXXX.ext` format\n",
    "pattern = re.compile(r'artwork\\d{3}\\.\\w+')\n",
    "\n",
    "for file_name in files:\n",
    "    file_ext = os.path.splitext(file_name)[1]\n",
    "    \n",
    "    old_file_path = os.path.join(image_dir, file_name)\n",
    "    new_file_path = old_file_path\n",
    "    \n",
    "    if not pattern.match(file_name):\n",
    "        new_file_name = f'artwork{counter:03d}{file_ext}'\n",
    "        new_file_path = os.path.join(image_dir, new_file_name)\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        counter += 1\n",
    "    \n",
    "    try:\n",
    "        # Open the image\n",
    "        with Image.open(new_file_path) as img:\n",
    "            # Check if the image size is not 128x128\n",
    "            if img.size != (128, 128):\n",
    "                img_resized = img.resize((128, 128))\n",
    "                img_resized.save(new_file_path)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Skipping invalid file: {file_name}, error: {e}\")\n",
    "\n",
    "print(\"Renaming and resizing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and DataLoader created successfully.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, text_dir, image_dir, transform=None, tokenizer_path='tokenizer.json'):\n",
    "        self.text_dir = text_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.text_files = sorted([f for f in os.listdir(text_dir) if f.endswith('.txt')])\n",
    "        \n",
    "        # Load the tokenizer\n",
    "        with open(tokenizer_path, 'r') as f:\n",
    "            self.word_to_idx = json.load(f)\n",
    "        \n",
    "        self.valid_image_files = self.filter_valid_images()\n",
    "        \n",
    "        if len(self.text_files) != len(self.valid_image_files):\n",
    "            raise ValueError(\"Number of text files does not match number of valid image files.\")\n",
    "\n",
    "    def filter_valid_images(self):\n",
    "        valid_files = []\n",
    "        for img_name in self.image_files:\n",
    "            img_path = os.path.join(self.image_dir, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img.verify()  # Verify that it is, in fact, an image\n",
    "                valid_files.append(img_name)\n",
    "            except (UnidentifiedImageError, IOError):\n",
    "                print(f\"Invalid image file: {img_path}\")\n",
    "        return valid_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.valid_image_files[idx])\n",
    "        image = Image.open(img_name)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        txt_name = os.path.join(self.text_dir, self.text_files[idx])\n",
    "        description = self.read_text_file(txt_name)\n",
    "        \n",
    "        tokens = self.tokenize_description(description)\n",
    "        return image, tokens\n",
    "\n",
    "    def read_text_file(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='latin1') as f:\n",
    "                return f.read()\n",
    "    \n",
    "    def tokenize_description(self, description):\n",
    "        words = re.split(r',\\s*', description.lower())\n",
    "        tokens = [self.word_to_idx[word] for word in words if word in self.word_to_idx]\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset = TextImageDataset(text_dir='dataset/text', image_dir='dataset/images', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(\"Dataset and DataLoader created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Defining the Generator and Discriminator networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, text_dim, img_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.text_embedding = nn.Embedding(text_dim, latent_dim)\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim * 2, 512, 4, 1, 0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text):\n",
    "        text_embedding = self.text_embedding(text).sum(dim=1)\n",
    "        x = torch.cat([noise, text_embedding], dim=1)\n",
    "        x = x.unsqueeze(2).unsqueeze(3)\n",
    "        return self.gen(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, text_dim, hidden_dim, img_size=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def conv_output_size(size, kernel_size=4, stride=2, padding=1):\n",
    "            return (size - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        current_size = img_size\n",
    "        self.img_dis = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, hidden_dim // 4, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        current_size = conv_output_size(current_size)\n",
    "\n",
    "        self.img_dis.add_module(\"conv2\", nn.Conv2d(hidden_dim // 4, hidden_dim // 2, 4, stride=2, padding=1))\n",
    "        self.img_dis.add_module(\"lrelu2\", nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_size = conv_output_size(current_size)\n",
    "\n",
    "        self.img_dis.add_module(\"conv3\", nn.Conv2d(hidden_dim // 2, hidden_dim, 4, stride=2, padding=1))\n",
    "        self.img_dis.add_module(\"lrelu3\", nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_size = conv_output_size(current_size)\n",
    "\n",
    "        self.flattened_img_size = hidden_dim * (current_size * current_size)\n",
    "\n",
    "        self.text_embedding = nn.Embedding(text_dim, hidden_dim * (current_size * current_size))\n",
    "        self.final = nn.Linear(self.flattened_img_size + self.flattened_img_size, 1)\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        img_out = self.img_dis(img).view(img.size(0), -1)\n",
    "        text_embedding = self.text_embedding(text).sum(dim=1)\n",
    "        x = torch.cat([img_out, text_embedding], dim=1)\n",
    "        return self.final(x)\n",
    "\n",
    "text_dim = len(word_to_idx)  # Set the text dimension to the number of unique words\n",
    "latent_dim = 100\n",
    "hidden_dim = 512\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator = Generator(latent_dim, text_dim, 3).to(device)\n",
    "discriminator = Discriminator(3, text_dim, hidden_dim, img_size=128).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "The generator and discriminator are trained here using a loop.\n",
    "## Optimization\n",
    "The optimization is embedded within the training loop. The code uses Adam optimizer to update the weights of both the generator and the discriminator.\n",
    "## Evaluation\n",
    "Evaluation happens within the training loop where loss values for both the generator and the discriminator are printed every 100 steps. Additionally, generated sample images are saved at the end of each epoch to visualize the generatorâ€™s progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, epochs, lr, device):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optim_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, text_ids) in enumerate(dataloader):\n",
    "            batch_size = images.size(0)\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            images = images.to(device)\n",
    "            text_ids = text_ids.to(device)\n",
    "\n",
    "            optim_d.zero_grad()\n",
    "            outputs = discriminator(images, text_ids)\n",
    "            real_loss = criterion(outputs, real_labels)\n",
    "            real_loss.backward()\n",
    "\n",
    "            noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_images = generator(noise, text_ids)\n",
    "            outputs = discriminator(fake_images.detach(), text_ids)\n",
    "            fake_loss = criterion(outputs, fake_labels)\n",
    "            fake_loss.backward()\n",
    "            optim_d.step()\n",
    "\n",
    "            optim_g.zero_grad()\n",
    "            outputs = discriminator(fake_images, text_ids)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            g_loss.backward()\n",
    "            optim_g.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(dataloader)}], '\n",
    "                    f'D Loss: {real_loss.item() + fake_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample_noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "            sample_images = generator(sample_noise, text_ids)\n",
    "            os.makedirs('samples', exist_ok=True)\n",
    "            utils.save_image(sample_images, f'samples/sample_epoch_{epoch + 1}.png', nrow=8, normalize=True)\n",
    "\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
