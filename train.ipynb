{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the appropriate command for your operating system, if required\n",
    "# No installation is reqiured on Google Colab / Kaggle notebooks\n",
    "\n",
    "# Linux / Binder / Windows (No GPU)\n",
    "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Linux / Windows (GPU)\n",
    "# pip install numpy matplotlib torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    " \n",
    "# MacOS (NO GPU)\n",
    "# !pip install numpy matplotlib torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "required_packages = ['numpy', 'matplotlib', 'torch', 'torchvision', 'torchaudio', 'transformers', 'pillow', 'json']\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        install(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating a local tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(text_dir):\n",
    "    # extract unique keywords\n",
    "    unique_words = set()\n",
    "    text_files = [f for f in os.listdir(text_dir) if f.endswith('.txt')]\n",
    "    for file_name in text_files:\n",
    "        file_path = os.path.join(text_dir, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read().lower()\n",
    "            words = re.split(r',\\s*', content)\n",
    "            unique_words.update(words)\n",
    "    \n",
    "    # create a mapping from keywords to indices\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(sorted(unique_words))}\n",
    "    \n",
    "    # save the tokenizer for later use\n",
    "    with open('tokenizer.json', 'w') as f:\n",
    "        json.dump(word_to_idx, f)\n",
    "    \n",
    "    return word_to_idx\n",
    "\n",
    "word_to_idx = create_tokenizer('dataset/text')\n",
    "print(f\"Tokenized keywords: {word_to_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'dataset'\n",
    "image_folders = ['nomadic', 'cubism', 'print', 'gray']\n",
    "output_image_dir = os.path.join(base_dir, 'images')\n",
    "output_text_dir = os.path.join(base_dir, 'text')\n",
    "\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(output_text_dir, exist_ok=True)\n",
    "\n",
    "# list of filters and enhancements\n",
    "filters = [\n",
    "    ('BLUR', ImageFilter.BLUR),\n",
    "    ('CONTOUR', ImageFilter.CONTOUR),\n",
    "    ('DETAIL', ImageFilter.DETAIL),\n",
    "    ('EDGE_ENHANCE', ImageFilter.EDGE_ENHANCE),\n",
    "    ('EDGE_ENHANCE_MORE', ImageFilter.EDGE_ENHANCE_MORE),\n",
    "    ('EMBOSS', ImageFilter.EMBOSS),\n",
    "    ('FIND_EDGES', ImageFilter.FIND_EDGES),\n",
    "    ('SHARPEN', ImageFilter.SHARPEN),\n",
    "    ('SMOOTH', ImageFilter.SMOOTH),\n",
    "    ('SMOOTH_MORE', ImageFilter.SMOOTH_MORE)\n",
    "]\n",
    "enhancements = [\n",
    "    ('BRIGHTNESS', ImageEnhance.Brightness, 1.2),\n",
    "    ('CONTRAST', ImageEnhance.Contrast, 1.2),\n",
    "    ('COLOR', ImageEnhance.Color, 1.2),\n",
    "    ('SHARPNESS', ImageEnhance.Sharpness, 1.2)\n",
    "]\n",
    "additional_filters = [\n",
    "    ('GAUSSIAN_BLUR', ImageFilter.GaussianBlur(radius=2)),\n",
    "    ('UNSHARP_MASK', ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3)),\n",
    "    ('MEDIAN_FILTER', ImageFilter.MedianFilter(size=3)),\n",
    "    ('MIN_FILTER', ImageFilter.MinFilter(size=3)),\n",
    "    ('MAX_FILTER', ImageFilter.MaxFilter(size=3)),\n",
    "    ('MODE_FILTER', ImageFilter.ModeFilter(size=3))\n",
    "]\n",
    "\n",
    "all_operations = filters + enhancements + additional_filters\n",
    "\n",
    "def apply_operations(image, output_dir, base_filename, counter):\n",
    "    original_image_path = os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\")\n",
    "    image.save(original_image_path)\n",
    "    counter += 1\n",
    "\n",
    "    for operation_name, operation in filters:\n",
    "        modified_image = image.filter(operation)\n",
    "        modified_image.save(os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\"))\n",
    "        counter += 1\n",
    "\n",
    "    for operation_name, operation, factor in enhancements:\n",
    "        enhancer = operation(image)\n",
    "        modified_image = enhancer.enhance(factor)\n",
    "        modified_image.save(os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\"))\n",
    "        counter += 1\n",
    "\n",
    "    for operation_name, operation in additional_filters:\n",
    "        modified_image = image.filter(operation)\n",
    "        modified_image.save(os.path.join(output_dir, f\"{base_filename}{counter:03d}.png\"))\n",
    "        counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "# function to delete all non-folder files in a directory\n",
    "def clean_directory(directory):\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        if os.path.isfile(item_path) and '.' in item:\n",
    "            os.remove(item_path)\n",
    "\n",
    "image_counter = 1\n",
    "\n",
    "clean_directory(output_image_dir)\n",
    "clean_directory(output_text_dir)\n",
    "\n",
    "for folder in image_folders:\n",
    "    current_folder_path = os.path.join(base_dir, 'images', folder)\n",
    "    for filename in os.listdir(current_folder_path):\n",
    "        if filename.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'gif')):\n",
    "            image_path = os.path.join(current_folder_path, filename)\n",
    "            image = Image.open(image_path)\n",
    "            base_filename = \"artwork\"\n",
    "\n",
    "            image_counter = apply_operations(image, output_image_dir, base_filename, image_counter)\n",
    "\n",
    "            # Create corresponding text file\n",
    "            for i in range(image_counter - 21, image_counter):\n",
    "                txt_filename = f\"{base_filename}{i:03d}.txt\"\n",
    "                with open(os.path.join(output_text_dir, txt_filename), 'w') as f:\n",
    "                    f.write(folder)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "image_dir = 'dataset/images'\n",
    "files = os.listdir(image_dir)  # list of all files in the directory\n",
    "\n",
    "counter = 1\n",
    "\n",
    "# pattern to match files already in the `artworkXXX.ext` format\n",
    "pattern = re.compile(r'artwork\\d{3}\\.\\w+')\n",
    "\n",
    "for file_name in files:\n",
    "    file_ext = os.path.splitext(file_name)[1]\n",
    "    \n",
    "    old_file_path = os.path.join(image_dir, file_name)\n",
    "    new_file_path = old_file_path\n",
    "    \n",
    "    if not pattern.match(file_name) and file_name not in ['nomadic', 'cubism', 'gray', 'print']:\n",
    "        new_file_name = f'artwork{counter:03d}{file_ext}'\n",
    "        new_file_path = os.path.join(image_dir, new_file_name)\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        counter += 1\n",
    "    \n",
    "    try:\n",
    "        # Open the image\n",
    "        with Image.open(new_file_path) as img:\n",
    "            # Check if the image size is not 128x128\n",
    "            if img.size != (128, 128):\n",
    "                img_resized = img.resize((128, 128))\n",
    "                img_resized.save(new_file_path)\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Skipping invalid file: {file_name}, error: {e}\")\n",
    "\n",
    "print(\"Renaming and resizing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, text_dir, image_dir, transform=None, tokenizer_path='tokenizer.json'):\n",
    "        self.text_dir = text_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.text_files = sorted([f for f in os.listdir(text_dir) if f.endswith('.txt')])\n",
    "        \n",
    "        # Load the tokenizer\n",
    "        with open(tokenizer_path, 'r') as f:\n",
    "            self.word_to_idx = json.load(f)\n",
    "        \n",
    "        self.valid_image_files = self.filter_valid_images()\n",
    "        \n",
    "        if len(self.text_files) != len(self.valid_image_files):\n",
    "            raise ValueError(\"Number of text files does not match number of valid image files.\")\n",
    "\n",
    "    def filter_valid_images(self):\n",
    "        valid_files = []\n",
    "        for img_name in self.image_files:\n",
    "            img_path = os.path.join(self.image_dir, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img.verify()  # Verify that it is, in fact, an image\n",
    "                valid_files.append(img_name)\n",
    "            except (UnidentifiedImageError, IOError):\n",
    "                print(f\"Invalid image file: {img_path}\")\n",
    "        return valid_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.valid_image_files[idx])\n",
    "        image = Image.open(img_name)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        txt_name = os.path.join(self.text_dir, self.text_files[idx])\n",
    "        description = self.read_text_file(txt_name)\n",
    "        \n",
    "        tokens = self.tokenize_description(description)\n",
    "        return image, tokens\n",
    "\n",
    "    def read_text_file(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='latin1') as f:\n",
    "                return f.read()\n",
    "    \n",
    "    def tokenize_description(self, description):\n",
    "        words = re.split(r',\\s*', description.lower())\n",
    "        tokens = [self.word_to_idx[word] for word in words if word in self.word_to_idx]\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset = TextImageDataset(text_dir='dataset/text', image_dir='dataset/images', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "print(\"Dataset and DataLoader created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Defining the Generator and Discriminator networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, text_dim, img_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.text_embedding = nn.Embedding(text_dim, latent_dim)\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim * 2, 512, 4, 1, 0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text):\n",
    "        text_embedding = self.text_embedding(text).sum(dim=1)\n",
    "        x = torch.cat([noise, text_embedding], dim=1)\n",
    "        x = x.unsqueeze(2).unsqueeze(3)\n",
    "        return self.gen(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, text_dim, hidden_dim, img_size=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def conv_output_size(size, kernel_size=4, stride=2, padding=1):\n",
    "            return (size - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        current_size = img_size\n",
    "        self.img_dis = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, hidden_dim // 4, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        current_size = conv_output_size(current_size)\n",
    "\n",
    "        self.img_dis.add_module(\"conv2\", nn.Conv2d(hidden_dim // 4, hidden_dim // 2, 4, stride=2, padding=1))\n",
    "        self.img_dis.add_module(\"lrelu2\", nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_size = conv_output_size(current_size)\n",
    "\n",
    "        self.img_dis.add_module(\"conv3\", nn.Conv2d(hidden_dim // 2, hidden_dim, 4, stride=2, padding=1))\n",
    "        self.img_dis.add_module(\"lrelu3\", nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_size = conv_output_size(current_size)\n",
    "\n",
    "        self.flattened_img_size = hidden_dim * (current_size * current_size)\n",
    "\n",
    "        self.text_embedding = nn.Embedding(text_dim, hidden_dim * (current_size * current_size))\n",
    "        self.final = nn.Linear(self.flattened_img_size + self.flattened_img_size, 1)\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        img_out = self.img_dis(img).view(img.size(0), -1)\n",
    "        text_embedding = self.text_embedding(text).sum(dim=1)\n",
    "        x = torch.cat([img_out, text_embedding], dim=1)\n",
    "        return self.final(x)\n",
    "\n",
    "text_dim = len(word_to_idx)  # Set the text dimension to the number of unique words\n",
    "latent_dim = 100\n",
    "hidden_dim = 512\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator = Generator(latent_dim, text_dim, 3).to(device)\n",
    "discriminator = Discriminator(3, text_dim, hidden_dim, img_size=128).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "The generator and discriminator are trained here using a loop.\n",
    "## Optimization\n",
    "The optimization is embedded within the training loop. The code uses Adam optimizer to update the weights of both the generator and the discriminator.\n",
    "## Evaluation\n",
    "Evaluation happens within the training loop where loss values for both the generator and the discriminator are printed every 100 steps. Additionally, generated sample images are saved at the end of each epoch to visualize the generator’s progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "epochs = 5000  # number of epochs\n",
    "lr = 0.0001  # learning rate\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, epochs, lr, device):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optim_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, text_ids) in enumerate(dataloader):\n",
    "            batch_size = images.size(0)\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            images = images.to(device)\n",
    "            text_ids = text_ids.to(device)\n",
    "\n",
    "            optim_d.zero_grad()\n",
    "            outputs = discriminator(images, text_ids)\n",
    "            real_loss = criterion(outputs, real_labels)\n",
    "            real_loss.backward()\n",
    "\n",
    "            noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_images = generator(noise, text_ids)\n",
    "            outputs = discriminator(fake_images.detach(), text_ids)\n",
    "            fake_loss = criterion(outputs, fake_labels)\n",
    "            fake_loss.backward()\n",
    "            optim_d.step()\n",
    "\n",
    "            optim_g.zero_grad()\n",
    "            outputs = discriminator(fake_images, text_ids)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            g_loss.backward()\n",
    "            optim_g.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(dataloader)}], '\n",
    "                    f'D Loss: {real_loss.item() + fake_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample_noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "            sample_images = generator(sample_noise, text_ids)\n",
    "            os.makedirs('samples', exist_ok=True)\n",
    "            utils.save_image(sample_images, f'samples/sample_epoch_{epoch + 1}.png', nrow=8, normalize=True)\n",
    "\n",
    "    torch.save(generator.state_dict(), 'generator.pth')\n",
    "    torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
    "\n",
    "train_gan(generator, discriminator, dataloader, epochs, lr, device)           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
