{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the appropriate command for your operating system, if required\n",
    "# No installation is reqiured on Google Colab / Kaggle notebooks\n",
    "\n",
    "# Linux / Binder / Windows (No GPU)\n",
    "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Linux / Windows (GPU)\n",
    "# pip install numpy matplotlib torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    " \n",
    "# MacOS (NO GPU)\n",
    "# !pip install numpy matplotlib torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.11/site-packages (10.2.0)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "required_packages = ['numpy', 'matplotlib', 'torch', 'torchvision', 'torchaudio', 'transformers', 'pillow']\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        install(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot identify image file 'artwork001', skipping.\n",
      "Cannot identify image file '.DS_Store', skipping.\n",
      "Cannot identify image file 'artwork002', skipping.\n",
      "Renaming and conversion completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "image_dir = 'dataset/images'\n",
    "files = os.listdir(image_dir)  # list of all files in the directory\n",
    "\n",
    "counter = 1\n",
    "\n",
    "# pattern to match files already in the `artwork\\d{3}\\.\\w+` format\n",
    "pattern = re.compile(r'artwork\\d{3}\\.\\w+')\n",
    "\n",
    "for file_name in files:\n",
    "    file_ext = os.path.splitext(file_name)[1].lower()\n",
    "    \n",
    "    if not pattern.match(file_name) and file_ext != '.jpg':\n",
    "        new_file_name = f'artwork{counter:03d}.jpg'\n",
    "        \n",
    "        old_file_path = os.path.join(image_dir, file_name)\n",
    "        new_file_path = os.path.join(image_dir, new_file_name)\n",
    "        \n",
    "        try:\n",
    "            # Open the image, convert to RGB, resize, and save as JPG\n",
    "            with Image.open(old_file_path) as img:\n",
    "                img = img.convert('RGB')  # Convert to RGB\n",
    "                img = img.resize((128, 128))  # Resize to 128x128\n",
    "                img.save(new_file_path, 'JPEG')\n",
    "            counter += 1\n",
    "        except UnidentifiedImageError:\n",
    "            print(f\"Cannot identify image file '{file_name}', skipping.\")\n",
    "        \n",
    "print(\"Renaming and conversion completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid image file: dataset/images/artwork001\n",
      "Invalid image file: dataset/images/artwork002\n",
      "Invalid image file: dataset/images/artwork070.jxl\n"
     ]
    }
   ],
   "source": [
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "# custom dataset\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, text_dir, image_dir, transform=None):\n",
    "        self.text_dir = text_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.text_files = sorted(os.listdir(text_dir))\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.valid_image_files = self.filter_valid_images()\n",
    "\n",
    "    def filter_valid_images(self):\n",
    "        valid_files = []\n",
    "        for img_name in self.image_files:\n",
    "            img_path = os.path.join(self.image_dir, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img.verify()  # Verify that it is, in fact, an image\n",
    "                valid_files.append(img_name)\n",
    "            except (UnidentifiedImageError, IOError):\n",
    "                print(f\"Invalid image file: {img_path}\")\n",
    "        return valid_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.valid_image_files[idx])\n",
    "        image = Image.open(img_name)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        txt_name = os.path.join(self.text_dir, self.text_files[idx])\n",
    "        description = self.read_text_file(txt_name)\n",
    "        \n",
    "        tokens = self.tokenizer(description, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        return image, tokens['input_ids'].squeeze(), tokens['attention_mask'].squeeze()\n",
    "\n",
    "    def read_text_file(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='latin1') as f:\n",
    "                return f.read()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize expects 3 channels (RGB)\n",
    "])\n",
    "\n",
    "# creating dataset and dataloader\n",
    "dataset = TextImageDataset(text_dir='dataset/text', image_dir='dataset/images', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, text_dim, img_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.text_embedding = nn.Linear(text_dim, latent_dim)\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text):\n",
    "        text = text.float()\n",
    "        text_embedding = self.text_embedding(text)\n",
    "        x = torch.cat([noise, text_embedding], dim=1)\n",
    "        x = x.unsqueeze(2).unsqueeze(3)\n",
    "        return self.gen(x)\n",
    "\n",
    "# defining the Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, text_dim, hidden_dim, img_size=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Calculate the size of the feature maps after each convolution layer\n",
    "        def conv_output_size(size, kernel_size=4, stride=2, padding=1):\n",
    "            return (size - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        current_size = img_size\n",
    "        self.img_dis = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, hidden_dim // 4, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        current_size = conv_output_size(current_size)\n",
    "\n",
    "        self.img_dis.add_module(\"conv2\", nn.Conv2d(hidden_dim // 4, hidden_dim // 2, 4, stride=2, padding=1))\n",
    "        self.img_dis.add_module(\"lrelu2\", nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_size = conv_output_size(current_size)\n",
    "\n",
    "        self.img_dis.add_module(\"conv3\", nn.Conv2d(hidden_dim // 2, hidden_dim, 4, stride=2, padding=1))\n",
    "        self.img_dis.add_module(\"lrelu3\", nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_size = conv_output_size(current_size)\n",
    "\n",
    "        self.text_embedding = nn.Linear(text_dim, hidden_dim)\n",
    "        self.final = nn.Linear(hidden_dim * (current_size * current_size) + hidden_dim, 1)  # dynamically calculated\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        img_out = self.img_dis(img).view(img.size(0), -1)  # flatten the output\n",
    "        text_embedding = self.text_embedding(text.float())  # ensure text is float\n",
    "        x = torch.cat([img_out, text_embedding], dim=1)\n",
    "        return self.final(x)\n",
    "\n",
    "text_dim = 768  # BERT base model output dimension\n",
    "latent_dim = 100\n",
    "hidden_dim = 512  # to be adjusted as needed\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator = Generator(latent_dim, text_dim, 3).to(device)\n",
    "discriminator = Discriminator(3, text_dim, hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "The generator and discriminator are trained here using a loop.\n",
    "## Optimization\n",
    "The optimization is embedded within the training loop. The code uses Adam optimizer to update the weights of both the generator and the discriminator.\n",
    "## Evaluation\n",
    "Evaluation happens within the training loop where loss values for both the generator and the discriminator are printed every 100 steps. Additionally, generated sample images are saved at the end of each epoch to visualize the generator’s progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [100, 512, 4, 4], expected input[128, 200, 1, 1] to have 100 channels, but got 200 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator(\u001b[38;5;241m3\u001b[39m, text_dim, hidden_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# training the models\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m train_gan(generator, discriminator, dataloader, epochs, lr, device)\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, dataloader, epochs, lr, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m real_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, latent_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 22\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m generator(noise, text_ids)\n\u001b[1;32m     23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m discriminator(fake_images\u001b[38;5;241m.\u001b[39mdetach(), text_ids)\n\u001b[1;32m     24\u001b[0m fake_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, fake_labels)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, noise, text)\u001b[0m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([noise, text_embedding], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:948\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    943\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    944\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    946\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv_transpose2d(\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[1;32m    950\u001b[0m     output_padding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [100, 512, 4, 4], expected input[128, 200, 1, 1] to have 100 channels, but got 200 channels instead"
     ]
    }
   ],
   "source": [
    "def train_gan(generator, discriminator, dataloader, epochs, lr, device):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optim_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, text_ids, text_mask) in enumerate(dataloader):\n",
    "            batch_size = images.size(0)\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)  # Define real labels\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)  # Define fake labels\n",
    "\n",
    "            images = images.to(device)\n",
    "            text_ids = text_ids.to(device)\n",
    "            text_mask = text_mask.to(device)\n",
    "\n",
    "            optim_d.zero_grad()\n",
    "            outputs = discriminator(images, text_ids)\n",
    "            real_loss = criterion(outputs, real_labels)\n",
    "            real_loss.backward()\n",
    "\n",
    "            noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_images = generator(noise, text_ids)\n",
    "            outputs = discriminator(fake_images.detach(), text_ids)\n",
    "            fake_loss = criterion(outputs, fake_labels)\n",
    "            fake_loss.backward()\n",
    "            optim_d.step()\n",
    "\n",
    "            optim_g.zero_grad()\n",
    "            outputs = discriminator(fake_images, text_ids)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            g_loss.backward()\n",
    "            optim_g.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(dataloader)}], '\n",
    "                      f'D Loss: {real_loss.item() + fake_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample_noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "            sample_images = generator(sample_noise, text_ids)\n",
    "            os.makedirs('samples', exist_ok=True)\n",
    "            utils.save_image(sample_images, f'samples/sample_epoch_{epoch + 1}.png', nrow=8, normalize=True)\n",
    "\n",
    "    torch.save(generator.state_dict(), 'generator.pth')\n",
    "    torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
    "\n",
    "# parameter setting\n",
    "image_size = 64  \n",
    "batch_size = 128\n",
    "epochs = 25\n",
    "lr = 0.0002\n",
    "\n",
    "# adjusting Initialization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "text_dim = 128  # BERT base model output dimension\n",
    "latent_dim = 100  # latent vector dimension\n",
    "hidden_dim = 128  # adjusted hidden dimension\n",
    "\n",
    "generator = Generator(latent_dim, text_dim, 3).to(device)\n",
    "discriminator = Discriminator(3, text_dim, hidden_dim).to(device)\n",
    "\n",
    "# training the models\n",
    "train_gan(generator, discriminator, dataloader, epochs, lr, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
